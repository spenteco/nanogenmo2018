{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the metadata for the corpus\n",
    "\n",
    "I really only use the author name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(catalog) 1161\n",
      "len(author_catalog) 482\n"
     ]
    }
   ],
   "source": [
    "import glob, codecs, re\n",
    "\n",
    "catalog = {}\n",
    "author_catalog = {}\n",
    "\n",
    "f = codecs.open('SCIFI_pg_catalog_012615_FINAL.csv', 'r', encoding='utf-8')\n",
    "for line in f.read().split('\\n'):\n",
    "    cols = line.strip().split('|')\n",
    "    if len(cols) > 1:\n",
    "        \n",
    "        author = cols[1]\n",
    "        title = cols[2]\n",
    "        file_name = cols[5]\n",
    "        if file_name.find('/') > -1:\n",
    "            file_name = file_name.split('/')[-1]\n",
    "        \n",
    "        catalog[file_name] = [author, title]\n",
    "        \n",
    "        try:\n",
    "            author_catalog[author].append([title, file_name])\n",
    "        except KeyError:\n",
    "            author_catalog[author] = [[title, file_name]]\n",
    "        \n",
    "f.close()\n",
    "\n",
    "print 'len(catalog)', len(catalog)\n",
    "print 'len(author_catalog)', len(author_catalog)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.9.0\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en')\n",
    "\n",
    "print spacy.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split the corpus into sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processed 0\n",
      "processed 100\n",
      "processed 200\n",
      "processed 300\n",
      "processed 400\n",
      "processed 500\n",
      "processed 600\n",
      "processed 700\n",
      "processed 800\n",
      "processed 900\n",
      "processed 1000\n",
      "processed 1100\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "import glob, codecs, re\n",
    "\n",
    "f = codecs.open('SCIFI_sentences.csv', 'w', encoding='utf-8')\n",
    "for n, path_to_file in enumerate(glob.glob('scifi/*.txt')):\n",
    "    \n",
    "    if n % 100 == 0:\n",
    "        print 'processed', n\n",
    "    \n",
    "    file_name = path_to_file.split('/')[-1]\n",
    "    author = catalog[file_name][0]\n",
    "    title = catalog[file_name][1]\n",
    "    \n",
    "    text = re.sub('\\s+', ' ', codecs.open(path_to_file, 'r', encoding='utf-8').read())\n",
    "    \n",
    "    doc = nlp(text)\n",
    "    for s in doc.sents:\n",
    "        f.write(file_name + '|' + author + '|' + title + '|' + s.text.strip() + '\\n')\n",
    "    \n",
    "f.close()\n",
    "    \n",
    "print 'Done!'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gather up each author's sentences\n",
    "\n",
    "Note that for *Astounding Stories*, the author is always \"Various\".  Which means that the cliche detector doesn't quite work correctly . . ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!\n"
     ]
    }
   ],
   "source": [
    "import codecs, re\n",
    "\n",
    "author_text = {}\n",
    "\n",
    "f = codecs.open('SCIFI_sentences.csv', 'r', encoding='utf-8')\n",
    "for line in f.read().split('\\n'):\n",
    "    cols = line.strip().split('|')\n",
    "    if len(cols) > 1:\n",
    "    \n",
    "        file_name = cols[0]\n",
    "        author = cols[1]\n",
    "        title = cols[2]\n",
    "        text = cols[3]\n",
    "        \n",
    "        try:\n",
    "             author_text[author].append(text)\n",
    "        except KeyError:\n",
    "             author_text[author] = [text]\n",
    "    \n",
    "f.close()\n",
    "\n",
    "print 'Done!'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "velvet\n"
     ]
    }
   ],
   "source": [
    "a = 'velvetness'\n",
    "print a[:-4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the \"author frequency\" for the ngrams in the corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(ngram_author_counts) 588671\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from collections import defaultdict, Counter\n",
    "\n",
    "def fix_key(k):\n",
    "    \n",
    "    new_k = k\n",
    "    \n",
    "    #if len(new_k) > 3 and new_k.endswith('y'):\n",
    "    #    new_k = new_k[:-1]\n",
    "    #elif new_k.endswith('ness'):\n",
    "    #    new_k = new_k[:-4]\n",
    "        \n",
    "    return new_k\n",
    "\n",
    "# ----------------------------------------------------------------\n",
    "\n",
    "ngram_author_counts = defaultdict(int)\n",
    "ngram_author_sentence_xref = defaultdict(list)\n",
    "\n",
    "for author in author_text:\n",
    "    \n",
    "    all_author_ngrams = {}\n",
    "    \n",
    "    for text_n, text in enumerate(author_text[author]):\n",
    "            \n",
    "        doc = nlp(text)\n",
    "        \n",
    "        for a in range(0, len(doc) - 1):\n",
    "            if doc[a].tag_[0] in ['N', 'J'] and doc[a + 1].tag_[0] in ['N', 'J']:\n",
    "                \n",
    "                # I DON'T WANT SOME NGRAMS; \"Chapter II\" is not useful, and spacy thinks \"*\" is a noun.\n",
    "                # THIS MIGHT EVOLVE INTO A SECONDARY SET OF STOPWORDS?\n",
    "                \n",
    "                if doc[a].text.lower() != 'chapter' and doc[a + 1].text.lower() != 'chapter' and \\\n",
    "                    doc[a].text.lower() != '*' and doc[a + 1].text.lower() != '*':\n",
    "                        \n",
    "                    k1 = fix_key(doc[a].lemma_.lower())\n",
    "                    k2 = fix_key(doc[a + 1].lemma_.lower())\n",
    "                    \n",
    "                    keys = [k1, k2]\n",
    "                    #keys.sort()\n",
    "                    \n",
    "                    all_author_ngrams[' '.join(keys)] = 1\n",
    "                    \n",
    "                    ngram_author_sentence_xref[' '.join(keys)].append([author, text_n])\n",
    "            \n",
    "    for ngram in all_author_ngrams.keys():\n",
    "        ngram_author_counts[ngram] += 1\n",
    "\n",
    "print 'len(ngram_author_counts)', len(ngram_author_counts)\n",
    "print 'Done!' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "velvet blackness 2\n",
      "velvet sky 6\n",
      "velvet shadow 4\n",
      "velvety darkness 3\n",
      "velvet backdrop 3\n",
      "black velvet 33\n",
      "velvet black 7\n",
      "velvet night 5\n",
      "velvety blackness 2\n",
      "dusky velvet 1\n",
      "velvet dark 4\n",
      "velvety night 1\n"
     ]
    }
   ],
   "source": [
    "#for k, v in ngram_author_counts.iteritems():\n",
    "#    if 'velvet' in k and v > 1:\n",
    "#        print k, v\n",
    "        \n",
    "print\n",
    "for k, v in ngram_author_counts.iteritems():\n",
    "    if 'velvet' in k:\n",
    "        has_word = False\n",
    "        for w in ['black', 'sky', 'shadow', 'dark', 'black', 'backdrop', 'night']:\n",
    "            if w in k:\n",
    "                has_word = True\n",
    "                break\n",
    "        if has_word == True:\n",
    "            print k, v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "f = open('TEMP_ngram_author_counts.pickle', 'w')\n",
    "pickle.dump(ngram_author_counts, f)\n",
    "f.close()\n",
    "\n",
    "f = open('TEMP_ngram_author_sentence_xref.pickle', 'w')\n",
    "pickle.dump(ngram_author_sentence_xref, f)\n",
    "f.close()\n",
    "\n",
    "print 'Done!'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "f = open('TEMP_ngram_author_counts.pickle', 'r')\n",
    "ngram_author_counts = pickle.load(f)\n",
    "f.close()\n",
    "\n",
    "f = open('TEMP_ngram_author_sentence_xref.pickle', 'r')\n",
    "ngram_author_sentence_xref = pickle.load(f)\n",
    "f.close()\n",
    "\n",
    "print 'Done!'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word frequencies . . . "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(word_frequencies) 105028\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from collections import defaultdict, Counter\n",
    "\n",
    "word_counts = defaultdict(int)\n",
    "total_words = 0\n",
    "\n",
    "for author, texts in author_text.iteritems():\n",
    "    for text in texts:\n",
    "        for t in re.split('[^a-z]', text.lower()):\n",
    "            if t > '':\n",
    "                word_counts[t] += 1\n",
    "                total_words += 1\n",
    "                \n",
    "word_frequencies = {}\n",
    "for word, n in word_counts.iteritems():\n",
    "    word_frequencies[word] = float(n) / float(total_words)\n",
    "\n",
    "print 'len(word_frequencies)', len(word_frequencies)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find the cliches\n",
    "\n",
    "Here, I define a cliche as an ngram which is used by more than 30 authors, and which is composed of low-frequency words.\n",
    "\n",
    "I take the top 200 such ngrams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(selected_cliches) 200\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict, Counter\n",
    "\n",
    "weighted_cliches = []\n",
    "\n",
    "for w in Counter(ngram_author_counts).most_common():\n",
    "        \n",
    "    try:\n",
    "        w0_frequency = word_frequencies[w[0].split(' ')[0]]\n",
    "        w1_frequency = word_frequencies[w[0].split(' ')[1]]\n",
    "\n",
    "        low_frequency = w0_frequency\n",
    "        if w1_frequency < low_frequency:\n",
    "            low_frequency = w1_frequency\n",
    "\n",
    "        weighted_cliches.append([low_frequency, w[0], w[1]])\n",
    "    except KeyError:\n",
    "        pass\n",
    "            \n",
    "weighted_cliches.sort()\n",
    "\n",
    "selected_cliches = []\n",
    "\n",
    "for n, c in enumerate(weighted_cliches):\n",
    "    \n",
    "    if 'velvet' in c[1]:\n",
    "        has_word = False\n",
    "        for w in ['black', 'sky', 'shadow', 'dark', 'black', 'backdrop', 'night']:\n",
    "            if w in c[1]:\n",
    "                has_word = True\n",
    "                break\n",
    "        if has_word == True:\n",
    "            selected_cliches.append(c[1])\n",
    "    else:\n",
    "        if c[2] > 30: \n",
    "            if len(selected_cliches) < 200:\n",
    "                if c[1] not in ['amazing stories', 'end transcriber', 'extensive research', \n",
    "                             'minor spelling', 'science fiction', 'typographical error',\n",
    "                             'los angeles']:\n",
    "                    selected_cliches.append(c[1])\n",
    "    \n",
    "print 'len(selected_cliches)', len(selected_cliches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'administration building', u'aged man', u'alpha centauri', u'anything unusual', u'artificial gravity', u'atom bomb', u'atomic bomb', u'atomic energy', u'atomic power', u'average man', u'awful lot', u'bald head', u'beady eye', u'bearded man', u'black velvet', u'blank wall', u'blond hair', u'brave man', u'breast pocket', u'broad daylight', u'capital city', u'carbon dioxide', u'certain amount', u'civil war', u'clenched fist', u'cloudless sky', u'coat pocket', u'cold sweat', u'comfortable chair', u'conference room', u'conscious mind', u'considerable distance', u'considerable time', u'control panel', u'crew member', u'damned thing', u'desk drawer', u'desperate effort', u'dining room', u'dusky velvet', u'early dawn', u'eastern horizon', u'elderly man', u'electric current', u'evening meal', u'excited voice', u'fairy tale', u'fantastic universe', u'fat man', u'few inch', u'few month', u'few pace', u'few yard', u'fifth avenue', u'first glimpse', u'first impression', u'first opportunity', u'first visit', u'fist clenched', u'fourth day', u'fourth dimension', u'fuel tank', u'funny thing', u'gloved hand', u'good bet', u'good faith', u'good fortune', u'good health', u'good heaven', u'good humor', u'great adventure', u'great britain', u'great bulk', u'great difficulty', u'great discovery', u'great importance', u'grim smile', u'grown man', u'guinea pig', u'haired man', u'handsome face', u'hiding place', u'high degree', u'hind leg', u'hoarse voice', u'honest man', u'hotel room', u'immediate danger', u'inch high', u'inch long', u'inch tall', u'inch thick', u'instrument panel', u'intelligent being', u'intelligent life', u'intelligent race', u'interstellar space', u'keen eye', u'large scale', u'large sum', u'last month', u'last resort', u'little difficulty', u'long leg', u'long pause', u'long row', u'loud voice', u'low lip', u'machine shop', u'magnetic field', u'many generation', u'many month', u'medical man', u'metal plate', u'milky way', u'molten metal', u'naked eye', u'nervous system', u'new arrival', u'new discovery', u'new jersey', u'next month', u'nineteenth century', u'north america', u'north pole', u'nothing unusual', u'old chap', u'old gentleman', u'open doorway', u'other member', u'other nation', u'other passenger', u'outstretched arm', u'outstretched hand', u'own accord', u'own affair', u'own fault', u'own sake', u'own tongue', u'own volition', u'patrol ship', u'per cent', u'phone call', u'planetary system', u'practical joke', u'practical purpose', u'precious stone', u'previous day', u'previous night', u'prime minister', u'public opinion', u'pure white', u'puzzled look', u'raw material', u'red lip', u'regular interval', u'rich man', u'right angle', u'right leg', u'right track', u'salt water', u'san francisco', u'scientific research', u'several month', u'shoulder blade', u'shrill cry', u'single file', u'small amount', u'something unusual', u'sound asleep', u'south america', u'south pole', u'split second', u'square inch', u'thin lip', u'tidal wave', u'total darkness', u'tree trunk', u'twentieth century', u'upper lip', u'utter silence', u'vague idea', u'vantage point', u'velvet backdrop', u'velvet black', u'velvet blackness', u'velvet dark', u'velvet night', u'velvet shadow', u'velvet sky', u'velvety blackness', u'velvety darkness', u'velvety night', u'west coast', u'white beard', u'white tooth', u'whole affair', u'wild beast', u'wise man', u'wrist watch']\n"
     ]
    }
   ],
   "source": [
    "print sorted(selected_cliches)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find sentences which contain at least once cliche."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!\n"
     ]
    }
   ],
   "source": [
    "import codecs\n",
    "\n",
    "f = codecs.open('1_selected_sentences.txt', 'w', encoding='utf-8')\n",
    "\n",
    "for c in selected_cliches:\n",
    "    \n",
    "    for author_sentence in ngram_author_sentence_xref[c]:\n",
    "        if 'copyright' not in author_text[author_sentence[0]][author_sentence[1]]:\n",
    "            f.write(c + '|' + author_text[author_sentence[0]][author_sentence[1]] + '\\n')\n",
    "    \n",
    "f.close()\n",
    "\n",
    "print 'Done!'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!sort 1_selected_sentences.txt | uniq > 1_selected_sentences.UNIQ.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
