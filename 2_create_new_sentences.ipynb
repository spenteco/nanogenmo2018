{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A markov-chain sentence generator\n",
    "\n",
    "There's some cruft here (like the bit to not take sentences with \"PERSON\", \"LOCATION\", \"ORGANIZATION\").  These things happen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(lines) 16136\n",
      "len(sentences) 15743\n"
     ]
    }
   ],
   "source": [
    "import codecs, re\n",
    "\n",
    "def decruft_sentences(path_to_file):\n",
    "\n",
    "    sentences = []\n",
    "    \n",
    "    lines = codecs.open(path_to_file, 'r', encoding='utf-8').read().split('\\n')\n",
    "    \n",
    "    print 'len(lines)', len(lines)\n",
    "    \n",
    "    for line in lines:\n",
    "        if line.strip() > '':\n",
    "            cliche = line.strip().split('|')[0]\n",
    "            sentence = line.strip().split('|')[1]\n",
    "            \n",
    "            sentence = re.sub('\\* ', '', sentence)\n",
    "            sentence = re.sub('\\\"', '', sentence)\n",
    "            sentence = re.sub('\\[Illustration\\]', '', sentence)\n",
    "            \n",
    "            drop_this_one = False\n",
    "            if 'chapter' in sentence.lower() or '..............' in sentence.lower():\n",
    "                drop_this_one = True\n",
    "            elif '[' in sentence.lower():\n",
    "                drop_this_one = True\n",
    "            \n",
    "            if drop_this_one == False:\n",
    "                \n",
    "                tokens = re.split('[^A-Z]', sentence)\n",
    "                for t in tokens:\n",
    "                    if len(t) > 1 and t.upper() == t and t not in ['PERSON', 'LOCATION', 'ORGANIZATION']:\n",
    "                        drop_this_one = True\n",
    "                        break\n",
    "            \n",
    "            if drop_this_one == False:\n",
    "                \n",
    "                sentences.append(re.sub('\\s+', ' ', sentence.strip()))\n",
    "\n",
    "    print 'len(sentences)', len(sentences)\n",
    "    \n",
    "    return sentences\n",
    "\n",
    "original_sentences = decruft_sentences('1_selected_sentences.UNIQ.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(lines) 16136\n",
      "len(sentences) 15743\n",
      "len(original_sentences) 15534\n",
      "len(markov_chain) 194986\n",
      "len(starting_points) 2134\n"
     ]
    }
   ],
   "source": [
    "import codecs, re\n",
    "from collections import defaultdict\n",
    "\n",
    "KEY_LENGTH = 2\n",
    "\n",
    "original_sentences = decruft_sentences('1_selected_sentences.UNIQ.txt')\n",
    "\n",
    "markov_chain = defaultdict(lambda : defaultdict(int))\n",
    "\n",
    "for s in original_sentences:\n",
    "    \n",
    "    tokens = ['START_SENTENCE']\n",
    "    \n",
    "    cleaned_s = s.replace('\"', '').replace('[', '').replace(']', '').replace('*', ' ')\n",
    "    cleaned_s = re.sub('\\s+', ' ', cleaned_s.strip())\n",
    "    \n",
    "    for t in re.split('\\s+', cleaned_s):\n",
    "        if t > '':\n",
    "            tokens.append(t)\n",
    "    tokens.append('END_SENTENCE')\n",
    "    \n",
    "    for a in range(0, len(tokens) - KEY_LENGTH):\n",
    "        markov_chain[tuple(tokens[a: a + KEY_LENGTH])][tokens[a + KEY_LENGTH]] += 1\n",
    "\n",
    "starting_points = []\n",
    "for k in markov_chain:\n",
    "    if k[0] == 'START_SENTENCE':\n",
    "        starting_points.append(k)\n",
    "        \n",
    "original_sentences = set(original_sentences)\n",
    "\n",
    "print 'len(original_sentences)', len(original_sentences)\n",
    "print 'len(markov_chain)', len(markov_chain)\n",
    "print 'len(starting_points)', len(starting_points)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "NUMBER_OF_SENTENCES_TO_CREATE = 10000\n",
    "\n",
    "new_sentences = []\n",
    "\n",
    "while len(new_sentences) < NUMBER_OF_SENTENCES_TO_CREATE:\n",
    "\n",
    "    markov_key = random.sample(starting_points, 1)[0]\n",
    "\n",
    "    result_tokens = [markov_key[1]]\n",
    "\n",
    "    n_times_in_token_loop = 0\n",
    "    token_loop_okay = True\n",
    "\n",
    "    while True:\n",
    "\n",
    "        # I DON'T WANT TO ENDLESSLY CIRCLE IN THIS LOOP\n",
    "        n_times_in_token_loop += 1\n",
    "        if n_times_in_token_loop > 500:\n",
    "            token_loop_okay = False\n",
    "            break\n",
    "\n",
    "        possible_next_tokens = []\n",
    "        for token, occurrences in markov_chain[markov_key].iteritems():\n",
    "            for a in range(0, occurrences):\n",
    "                possible_next_tokens.append(token)\n",
    "\n",
    "        next_token = random.sample(possible_next_tokens, 1)[0]\n",
    "\n",
    "        if next_token == 'END_SENTENCE':\n",
    "            break\n",
    "        else:\n",
    "            result_tokens.append(next_token)\n",
    "\n",
    "        markov_key = tuple(list(markov_key[1:]) + [next_token])\n",
    "\n",
    "    result_sentence = ' '.join(result_tokens)\n",
    "\n",
    "    if token_loop_okay == True and \\\n",
    "        result_sentence not in original_sentences and \\\n",
    "        result_sentence not in new_sentences:\n",
    "        \n",
    "        new_sentences.append(result_sentence)\n",
    "\n",
    "f = codecs.open('2_markov_sentences.txt', 'w', encoding='utf-8')\n",
    "for s in new_sentences:\n",
    "    f.write(s + '\\n')\n",
    "f.close()\n",
    "\n",
    "print 'Done!'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2500 2_markov_sentences.txt\n",
      "65050 2_markov_sentences.txt\n"
     ]
    }
   ],
   "source": [
    "!wc -l 2_markov_sentences.txt\n",
    "!wc -w 2_markov_sentences.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
