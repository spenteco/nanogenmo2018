{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Force all person names to be one of six names\n",
    "\n",
    "The source data came from 1,161 texts, so the generated sentences contained a welter of character names.  To make the result seem a little more consistent, I run the generated sentences through the Stanford corenlp package (Stanford's named-entity recognition worked much better here than spacy's), map the resulting names to my six character names, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(stories) 100\n",
      "58.txt\n"
     ]
    }
   ],
   "source": [
    "import codecs, re, glob\n",
    "\n",
    "stories = []\n",
    "file_names = []\n",
    "\n",
    "for p in glob.glob('raw_stories/*.txt'):\n",
    "    file_names.append(p.split('/')[-1])\n",
    "    stories.append(re.split('\\n', codecs.open(p, 'r', encoding='utf-8').read()))\n",
    "    \n",
    "print 'len(stories)', len(stories)\n",
    "print file_names[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import re, commands, codecs\n",
    "from collections import defaultdict, Counter\n",
    "\n",
    "def process_story(s):\n",
    "\n",
    "    f = codecs.open('temp_sentences.txt', 'w', encoding='utf-8')\n",
    "    f.write('\\n'.join(s) + '\\n')\n",
    "    f.close()\n",
    "\n",
    "    cmd_results = commands.getoutput('cd /home/spenteco/0/stanford-corenlp-full-2018-10-05;' + \\\n",
    "                                './corenlp.sh -annotators tokenize,ssplit,pos,lemma,ner -outputFormat conll -file ' + \\\n",
    "                                 '/home/spenteco/1/nanogenmo_2017/temp_sentences.txt' + \\\n",
    "                                 ' -outputDirectory /home/spenteco/1/nanogenmo_2017')\n",
    "\n",
    "    f = codecs.open('temp_sentences.txt.conll', 'r', encoding='utf-8')\n",
    "    result_sentences =  re.split('\\n\\n', f.read())\n",
    "    f.close()\n",
    "    \n",
    "    persons = []\n",
    "    locations = []\n",
    "    organizations = []\n",
    "    \n",
    "    for r in result_sentences:\n",
    "    \n",
    "        name_type = ''\n",
    "        name_parts = []\n",
    "    \n",
    "        for t in r.split('\\n'):\n",
    "\n",
    "            cols = t.split('\\t')\n",
    "            if len(cols) > 4:\n",
    "\n",
    "                text = cols[1]\n",
    "                lemma = cols[2]\n",
    "                pos = cols[3]\n",
    "                ent = cols[4]\n",
    "            \n",
    "            if ent in ['PERSON', 'LOCATION', 'ORGANIZATION'] and \\\n",
    "                text not in ['\\'s', '--', 'bush', 'messenger', 'sun', 'earth', 'moon', 'of']:\n",
    "                    \n",
    "                if name_type > '' and name_type != ent:\n",
    "                \n",
    "                    if name_type == 'PERSON':\n",
    "                        persons.append(' '.join(name_parts))\n",
    "                    elif name_type == 'LOCATION':\n",
    "                        locations.append(' '.join(name_parts))\n",
    "                    elif name_type == 'ORGANIZATION':\n",
    "                        organizations.append(' '.join(name_parts))\n",
    "                        \n",
    "                    name_type = ''\n",
    "                    name_parts = []\n",
    "                    \n",
    "                else:\n",
    "                    name_parts.append(text)\n",
    "                    name_type = ent\n",
    "            else:\n",
    "                if name_type > '':\n",
    "                \n",
    "                    if name_type == 'PERSON':\n",
    "                        persons.append(' '.join(name_parts))\n",
    "                    elif name_type == 'LOCATION':\n",
    "                        locations.append(' '.join(name_parts))\n",
    "                    elif name_type == 'ORGANIZATION':\n",
    "                        organizations.append(' '.join(name_parts))\n",
    "                        \n",
    "                    name_type = ''\n",
    "                    name_parts = []\n",
    "    \n",
    "        \n",
    "    if name_type > '':\n",
    "\n",
    "        if name_type == 'PERSON':\n",
    "            persons.append(' '.join(name_parts))\n",
    "        elif name_type == 'LOCATION':\n",
    "            locations.append(' '.join(name_parts))\n",
    "        elif name_type == 'ORGANIZATION':\n",
    "            organizations.append(' '.join(name_parts))\n",
    "\n",
    "        name_type = ''\n",
    "        name_parts = []\n",
    "    \n",
    "    #persons = sorted(list(set(persons)))\n",
    "    #locations = sorted(list(set(locations)))\n",
    "    #organizations = sorted(list(set(organizations)))\n",
    "\n",
    "    return persons, locations, organizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Killian Masood', 'Killian'], ['Kamal Rainbird', 'Kamal'], ['Cyrus Rana', 'Cyrus'], ['Ace Kelly', 'Ace'], ['Rhea Landeros', 'Rhea'], ['Tekla Tanner', 'Tekla'], ['Ellia Alpha-2-Guthren', 'Ellia']]\n"
     ]
    }
   ],
   "source": [
    "# http://www.scifiideas.com/sci-fi-character-name-generator/\n",
    "\n",
    "import random\n",
    "\n",
    "raw_names = ['Killian Masood', 'Kamal Rainbird', 'Cyrus Rana', 'Ace Kelly', \n",
    "             'Rhea Landeros', 'Tekla Tanner', 'Ellia Alpha-2-Guthren']\n",
    "\n",
    "new_names = []\n",
    "for n in raw_names:\n",
    "    new_names.append([n, n.split(' ')[0]])\n",
    "\n",
    "print new_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99\n"
     ]
    }
   ],
   "source": [
    "import re, commands, codecs\n",
    "from collections import defaultdict, Counter\n",
    "    \n",
    "all_original_names = []\n",
    "    \n",
    "for sn, s in enumerate(stories):\n",
    "    \n",
    "    print sn,\n",
    "    \n",
    "    persons, locations, organizations = process_story(s)\n",
    "    \n",
    "    all_original_names.append(persons)\n",
    "    \n",
    "print"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'Mel', u'Dominico', u'Arek', u'Mel', u'Roger', u'Bastin', u'Bailey', u'Walters', u'Mel', u'Dominico', u'Nanlo', u'Nanlo', u'Mel', u'Merrywinkle', u'Masterson', u'Madonna']\n"
     ]
    }
   ],
   "source": [
    "print all_original_names[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(all_name_mappings) 100\n",
      "[[u'Mel', ['Ellia Alpha-2-Guthren', 'Ellia']], [u'Dominico', ['Rhea Landeros', 'Rhea']], [u'Nanlo', ['Ace Kelly', 'Ace']], [u'Bastin', ['Ellia Alpha-2-Guthren', 'Ellia']], [u'Masterson', ['Rhea Landeros', 'Rhea']], [u'Merrywinkle', ['Ace Kelly', 'Ace']], [u'Arek', ['Ellia Alpha-2-Guthren', 'Ellia']], [u'Roger', ['Rhea Landeros', 'Rhea']], [u'Bailey', ['Ace Kelly', 'Ace']], [u'Madonna', ['Ellia Alpha-2-Guthren', 'Ellia']], [u'Walters', ['Rhea Landeros', 'Rhea']]]\n"
     ]
    }
   ],
   "source": [
    "all_name_mappings = []\n",
    "\n",
    "for a, names in enumerate(all_original_names):\n",
    "    \n",
    "    name_counts = defaultdict(int)\n",
    "    for n in names:\n",
    "        name_counts[n] += 1\n",
    "    \n",
    "    random.shuffle(new_names)\n",
    "    \n",
    "    names_to_use = new_names[:3]\n",
    "    \n",
    "    new_i = -1\n",
    "    \n",
    "    name_mappings = []\n",
    "    \n",
    "    for w in Counter(name_counts).most_common():\n",
    "        new_i += 1\n",
    "        if new_i > len(names_to_use) - 1:\n",
    "            new_i = 0\n",
    "        name_mappings.append([w[0], names_to_use[new_i]])\n",
    "        \n",
    "        \n",
    "    all_name_mappings.append(name_mappings)\n",
    "    \n",
    "print 'len(all_name_mappings)', len(all_name_mappings)\n",
    "\n",
    "print all_name_mappings[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "for sn, s in enumerate(stories):\n",
    "    \n",
    "    named_story = []\n",
    "    last_names_used = []\n",
    "    \n",
    "    for sentence in s:\n",
    "        \n",
    "        new_sentence = sentence\n",
    "    \n",
    "        for name in all_name_mappings[sn]:\n",
    "\n",
    "            matches = []\n",
    "\n",
    "            for mn, m in enumerate(re.finditer(r'\\b' + name[0] + r'\\b', new_sentence)):\n",
    "\n",
    "                matches.append([m.start(), m.end()])\n",
    "\n",
    "            matches.reverse()\n",
    "\n",
    "            for mn, m in enumerate(matches):\n",
    "\n",
    "                if name[1][0] in last_names_used:\n",
    "                    new_sentence = new_sentence[:m[0]] + name[1][1] + new_sentence[m[1]:]\n",
    "                else:\n",
    "                    new_sentence = new_sentence[:m[0]] + name[1][0] + new_sentence[m[1]:]\n",
    "                    last_names_used.append(name[1][0])\n",
    "                    \n",
    "        named_story.append(new_sentence)\n",
    "                \n",
    "    stories[sn] = '\\n'.join(named_story)\n",
    "    \n",
    "print 'Done!'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!\n"
     ]
    }
   ],
   "source": [
    "for sn, s in enumerate(stories):\n",
    "    \n",
    "    f = codecs.open('named_stories/' + file_names[sn], 'w', encoding='utf-8')\n",
    "    f.write(stories[sn])\n",
    "    f.close()\n",
    "    \n",
    "print 'Done!'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
