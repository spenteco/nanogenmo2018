{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(markov_sentences) 10001\n"
     ]
    }
   ],
   "source": [
    "import codecs\n",
    "\n",
    "markov_sentences = codecs.open('2_markov_sentences.txt', 'r', encoding='utf-8').read().split('\\n')\n",
    "\n",
    "print 'len(markov_sentences)', len(markov_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.9.0\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en')\n",
    "\n",
    "print spacy.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transform the generated sentences  . . . \n",
    "\n",
    ". . . into lists of tokens, because I want to . . . "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " len(sentences_semantic_data) 10001\n"
     ]
    }
   ],
   "source": [
    "sentences_semantic_data = []\n",
    "\n",
    "for s in markov_sentences:\n",
    "    \n",
    "    semantic_data = []\n",
    "    \n",
    "    doc = nlp(unicode(s))\n",
    "    \n",
    "    for c in doc.noun_chunks:\n",
    "        \n",
    "        if len(c) == 1 and c[0].pos_ in ['PRON', 'PROPN']:\n",
    "            pass\n",
    "        else:\n",
    "            \n",
    "            all_pos = []\n",
    "            for t in c:\n",
    "                all_pos.append(t.pos_)\n",
    "            all_pos = list(set(all_pos))\n",
    "            \n",
    "            if len(all_pos) == 1 and all_pos[0] == 'PROPN':\n",
    "                pass\n",
    "            else:\n",
    "                semantic_data.append(c.text)\n",
    "                \n",
    "    for t in doc:\n",
    "        if t.pos_ == 'VERB':\n",
    "            semantic_data.append(t.lemma_)\n",
    "                \n",
    "    sentences_semantic_data.append(semantic_data) \n",
    "    \n",
    "print 'len(sentences_semantic_data)', len(sentences_semantic_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### . . . topic model the generated sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from gensim import corpora, models, similarities\n",
    "from gensim.models.wrappers import LdaMallet\n",
    "\n",
    "N_TOPICS = 100\n",
    "\n",
    "dictionary = corpora.Dictionary(sentences_semantic_data)\n",
    "corpus = [dictionary.doc2bow(text) for text in sentences_semantic_data]\n",
    "\n",
    "path_to_mallet_binary = '/home/spenteco/0/mallet-2.0.7/bin/mallet'\n",
    "\n",
    "model = LdaMallet(path_to_mallet_binary, \n",
    "                  corpus=corpus, \n",
    "                  num_topics=N_TOPICS, \n",
    "                  id2word=dictionary,\n",
    "                  optimize_interval=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lda_corpus = model[corpus]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get sentences for each topic\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 len(all_topic_sentences[a]) 15 422\n",
      "1 len(all_topic_sentences[a]) 13 314\n",
      "2 len(all_topic_sentences[a]) 15 446\n",
      "3 len(all_topic_sentences[a]) 31 799\n",
      "4 len(all_topic_sentences[a]) 30 784\n",
      "5 len(all_topic_sentences[a]) 30 477\n",
      "6 len(all_topic_sentences[a]) 23 494\n",
      "7 len(all_topic_sentences[a]) 16 332\n",
      "8 len(all_topic_sentences[a]) 25 494\n",
      "9 len(all_topic_sentences[a]) 21 468\n",
      "10 len(all_topic_sentences[a]) 20 529\n",
      "11 len(all_topic_sentences[a]) 18 344\n",
      "12 len(all_topic_sentences[a]) 46 1157\n",
      "13 len(all_topic_sentences[a]) 15 379\n",
      "14 len(all_topic_sentences[a]) 24 678\n",
      "15 len(all_topic_sentences[a]) 23 539\n",
      "16 len(all_topic_sentences[a]) 13 244\n",
      "17 len(all_topic_sentences[a]) 16 374\n",
      "18 len(all_topic_sentences[a]) 29 553\n",
      "19 len(all_topic_sentences[a]) 58 1717\n",
      "20 len(all_topic_sentences[a]) 22 556\n",
      "21 len(all_topic_sentences[a]) 34 599\n",
      "22 len(all_topic_sentences[a]) 34 813\n",
      "23 len(all_topic_sentences[a]) 17 512\n",
      "24 len(all_topic_sentences[a]) 40 689\n",
      "25 len(all_topic_sentences[a]) 37 753\n",
      "26 len(all_topic_sentences[a]) 56 895\n",
      "27 len(all_topic_sentences[a]) 18 307\n",
      "28 len(all_topic_sentences[a]) 27 552\n",
      "29 len(all_topic_sentences[a]) 19 544\n",
      "30 len(all_topic_sentences[a]) 30 635\n",
      "31 len(all_topic_sentences[a]) 15 326\n",
      "32 len(all_topic_sentences[a]) 23 430\n",
      "33 len(all_topic_sentences[a]) 17 419\n",
      "34 len(all_topic_sentences[a]) 28 590\n",
      "35 len(all_topic_sentences[a]) 32 886\n",
      "36 len(all_topic_sentences[a]) 17 305\n",
      "37 len(all_topic_sentences[a]) 17 577\n",
      "38 len(all_topic_sentences[a]) 27 794\n",
      "39 len(all_topic_sentences[a]) 22 646\n",
      "40 len(all_topic_sentences[a]) 29 590\n",
      "41 len(all_topic_sentences[a]) 19 455\n",
      "42 len(all_topic_sentences[a]) 27 693\n",
      "43 len(all_topic_sentences[a]) 32 855\n",
      "44 len(all_topic_sentences[a]) 22 512\n",
      "45 len(all_topic_sentences[a]) 13 362\n",
      "46 len(all_topic_sentences[a]) 19 316\n",
      "47 len(all_topic_sentences[a]) 36 544\n",
      "48 len(all_topic_sentences[a]) 20 571\n",
      "49 len(all_topic_sentences[a]) 28 620\n",
      "50 len(all_topic_sentences[a]) 25 564\n",
      "51 len(all_topic_sentences[a]) 21 393\n",
      "52 len(all_topic_sentences[a]) 20 464\n",
      "53 len(all_topic_sentences[a]) 17 344\n",
      "54 len(all_topic_sentences[a]) 94 2315\n",
      "55 len(all_topic_sentences[a]) 8 164\n",
      "56 len(all_topic_sentences[a]) 38 708\n",
      "57 len(all_topic_sentences[a]) 22 567\n",
      "58 len(all_topic_sentences[a]) 22 642\n",
      "59 len(all_topic_sentences[a]) 22 316\n",
      "60 len(all_topic_sentences[a]) 21 392\n",
      "61 len(all_topic_sentences[a]) 20 642\n",
      "62 len(all_topic_sentences[a]) 16 362\n",
      "63 len(all_topic_sentences[a]) 97 1687\n",
      "64 len(all_topic_sentences[a]) 19 479\n",
      "65 len(all_topic_sentences[a]) 25 523\n",
      "66 len(all_topic_sentences[a]) 29 540\n",
      "67 len(all_topic_sentences[a]) 24 609\n",
      "68 len(all_topic_sentences[a]) 43 1123\n",
      "69 len(all_topic_sentences[a]) 26 557\n",
      "70 len(all_topic_sentences[a]) 28 666\n",
      "71 len(all_topic_sentences[a]) 38 455\n",
      "72 len(all_topic_sentences[a]) 33 611\n",
      "73 len(all_topic_sentences[a]) 41 947\n",
      "74 len(all_topic_sentences[a]) 30 532\n",
      "75 len(all_topic_sentences[a]) 28 648\n",
      "76 len(all_topic_sentences[a]) 15 525\n",
      "77 len(all_topic_sentences[a]) 9 231\n",
      "78 len(all_topic_sentences[a]) 16 460\n",
      "79 len(all_topic_sentences[a]) 16 555\n",
      "80 len(all_topic_sentences[a]) 22 680\n",
      "81 len(all_topic_sentences[a]) 20 497\n",
      "82 len(all_topic_sentences[a]) 19 376\n",
      "83 len(all_topic_sentences[a]) 125 3263\n",
      "84 len(all_topic_sentences[a]) 19 455\n",
      "85 len(all_topic_sentences[a]) 27 599\n",
      "86 len(all_topic_sentences[a]) 46 680\n",
      "87 len(all_topic_sentences[a]) 33 638\n",
      "88 len(all_topic_sentences[a]) 25 566\n",
      "89 len(all_topic_sentences[a]) 32 606\n",
      "90 len(all_topic_sentences[a]) 26 401\n",
      "91 len(all_topic_sentences[a]) 28 775\n",
      "92 len(all_topic_sentences[a]) 22 522\n",
      "93 len(all_topic_sentences[a]) 27 680\n",
      "94 len(all_topic_sentences[a]) 21 468\n",
      "95 len(all_topic_sentences[a]) 14 383\n",
      "96 len(all_topic_sentences[a]) 29 597\n",
      "97 len(all_topic_sentences[a]) 28 797\n",
      "98 len(all_topic_sentences[a]) 19 439\n",
      "99 len(all_topic_sentences[a]) 22 540\n",
      "\n",
      "total_words 61873\n",
      "total_sentences 2725\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "all_topic_sentences = []\n",
    "\n",
    "for topic in range(0, N_TOPICS):\n",
    "    \n",
    "    topic_sentences = []\n",
    "    \n",
    "    for a in range(0, len(lda_corpus)):\n",
    "    \n",
    "        if lda_corpus[a][0][0] == topic and lda_corpus[a][0][1] > 0.40:\n",
    "        #if lda_corpus[a][0][0] == topic:\n",
    "            topic_sentences.append(markov_sentences[a])\n",
    "            \n",
    "    all_topic_sentences.append(topic_sentences)\n",
    "    \n",
    "total_words = 0\n",
    "total_sentences = 0\n",
    "\n",
    "for a in range(0, len(all_topic_sentences)):\n",
    "    \n",
    "    text = ' '.join(all_topic_sentences[a])\n",
    "    words = []\n",
    "    for t in re.split('[^A-Za-z]', text):\n",
    "        if t > '':\n",
    "            words.append(t)\n",
    "            \n",
    "    total_words += len(words)\n",
    "    total_sentences += len(all_topic_sentences[a])\n",
    "    \n",
    "    print a, 'len(all_topic_sentences[a])', len(all_topic_sentences[a]), len(words)\n",
    "    \n",
    "print\n",
    "print 'total_words', total_words\n",
    "print 'total_sentences', total_sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Just checking . . . do I actually have something?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0\n",
      "\n",
      "west coast 6\n",
      "dashes coast 3\n",
      "lightning dashes 3\n",
      "nasal drawl 2\n",
      "year drives 2\n",
      "\n",
      "1\n",
      "\n",
      "rising degrees 5\n",
      "vector rising 5\n",
      "degrees vector 5\n",
      "crawling things 3\n",
      "things like 3\n",
      "\n",
      "2\n",
      "\n",
      "hurrah last 4\n",
      "left alive 4\n",
      "last man 4\n",
      "man left 4\n",
      "alive hurrah 4\n",
      "\n",
      "3\n",
      "\n",
      "wrist watch 9\n",
      "black velvet 9\n",
      "velvet space 4\n",
      "interplanetary exchange 2\n",
      "arrived bearing 2\n",
      "\n",
      "4\n",
      "\n",
      "hiding place 14\n",
      "naked eye 6\n",
      "hiding places 3\n",
      "north america 2\n",
      "cement walk 2\n",
      "\n",
      "5\n",
      "\n",
      "north pole 13\n",
      "south pole 4\n",
      "north america 3\n",
      "reach north 3\n",
      "attempts reach 3\n",
      "\n",
      "6\n",
      "\n",
      "awful lot 6\n",
      "sir guy 2\n",
      "leather socket 2\n",
      "rouse pack 2\n",
      "roused voice 2\n",
      "\n",
      "7\n",
      "\n",
      "south pole 5\n",
      "gunpowder printing 3\n",
      "printing press 3\n",
      "press knows 2\n",
      "cement roadway 2\n",
      "\n",
      "8\n",
      "\n",
      "early dawn 6\n",
      "eventually advancing 3\n",
      "tens thousands 3\n",
      "san francisco 3\n",
      "solar system 3\n",
      "\n",
      "9\n",
      "\n",
      "doctor corner 3\n",
      "sort corn 3\n",
      "corn doctor 3\n",
      "land sunshine 3\n",
      "spell binder 3\n",
      "\n",
      "10\n",
      "\n",
      "self sufficient 3\n",
      "facility command 3\n",
      "like arm 3\n",
      "control self 3\n",
      "self service 3\n",
      "\n",
      "11\n",
      "\n",
      "adult life 4\n",
      "tabby adult 4\n",
      "spoken every 2\n",
      "successful communications 2\n",
      "conveyed every 2\n",
      "\n",
      "12\n",
      "\n",
      "per cent 3\n",
      "old gentleman 3\n",
      "remaining engine 2\n",
      "single remaining 2\n",
      "gentleman listened 2\n",
      "\n",
      "13\n",
      "\n",
      "century counselor 3\n",
      "second century 3\n",
      "twentieth century 3\n",
      "twenty second 3\n",
      "counselor sat 3\n",
      "\n",
      "14\n",
      "\n",
      "cloudless sky 4\n",
      "white hot 3\n",
      "kindling air 3\n",
      "air sky 3\n",
      "perfume mirror 3\n",
      "\n",
      "15\n",
      "\n",
      "patrol ship 13\n",
      "special patrol 5\n",
      "general practice 4\n",
      "administration building 4\n",
      "practice patrol 4\n",
      "\n",
      "16\n",
      "\n",
      "bellews would 2\n",
      "beach area 2\n",
      "densely populated 2\n",
      "recall dim 2\n",
      "populated beach 2\n",
      "\n",
      "17\n",
      "\n",
      "capital city 9\n",
      "claimed attention 2\n",
      "stretched largest 2\n",
      "city vicinity 2\n",
      "hat claimed 2\n",
      "\n",
      "18\n",
      "\n",
      "nervous system 15\n",
      "strengthens nervous 6\n",
      "formula strengthens 6\n",
      "ricky formula 6\n",
      "raw material 4\n",
      "\n",
      "19\n",
      "\n",
      "great difficulty 8\n",
      "milky way 5\n",
      "climbers found 4\n",
      "greatest adventure 4\n",
      "great discovery 4\n",
      "\n",
      "20\n",
      "\n",
      "suddenly assuming 3\n",
      "agreed suddenly 3\n",
      "gripping wrench 3\n",
      "right agreed 3\n",
      "nears american 2\n",
      "\n",
      "21\n",
      "\n",
      "atomic energy 18\n",
      "atomic power 7\n",
      "increased harnessing 5\n",
      "harnessing atomic 5\n",
      "fiction new 3\n",
      "\n",
      "22\n",
      "\n",
      "grim smile 10\n",
      "features became 4\n",
      "became even 4\n",
      "vall handsome 4\n",
      "handsome features 4\n",
      "\n",
      "23\n",
      "\n",
      "open doorway 5\n",
      "suppressed irritation 3\n",
      "list sextus 3\n",
      "rivardi inwardly 3\n",
      "movie stars 3\n",
      "\n",
      "24\n",
      "\n",
      "handsome face 18\n",
      "alden handsome 6\n",
      "face softened 4\n",
      "farrow face 4\n",
      "deepened suspicion 4\n",
      "\n",
      "25\n",
      "\n",
      "thin lips 15\n",
      "red lips 6\n",
      "efficient reporters 4\n",
      "nervously efficient 4\n",
      "reporters written 4\n",
      "\n",
      "26\n",
      "\n",
      "gloved hands 11\n",
      "gloved hand 10\n",
      "outstretched hand 5\n",
      "outstretched arms 4\n",
      "metal gloved 4\n",
      "\n",
      "27\n",
      "\n",
      "coat pocket 7\n",
      "breast pocket 4\n",
      "inside coat 3\n",
      "wiped hatband 2\n",
      "pocket daddy 2\n",
      "\n",
      "28\n",
      "\n",
      "good heavens 11\n",
      "tweel feathery 4\n",
      "feathery appendages 4\n",
      "heavens said 3\n",
      "appendages consultations 2\n",
      "\n",
      "29\n",
      "\n",
      "certain amount 4\n",
      "safe spots 2\n",
      "spots store 2\n",
      "price tag 2\n",
      "correctly interpreted 2\n",
      "\n",
      "30\n",
      "\n",
      "trembling hands 7\n",
      "cold sweat 7\n",
      "steadied upon 6\n",
      "hands steadied 6\n",
      "close set 5\n",
      "\n",
      "31\n",
      "\n",
      "shoulder blades 5\n",
      "fuel tank 4\n",
      "tank exploded 3\n",
      "calmly talked 2\n",
      "flame rifles 2\n",
      "\n",
      "32\n",
      "\n",
      "carbon dioxide 12\n",
      "yellow strips 4\n",
      "hurrying bark 3\n",
      "busy citizens 3\n",
      "absorbers tanks 3\n",
      "\n",
      "33\n",
      "\n",
      "guess wear 4\n",
      "wear fur 4\n",
      "fur suit 4\n",
      "affectionate mimicry 3\n",
      "oh yes 3\n",
      "\n",
      "34\n",
      "\n",
      "white clothed 6\n",
      "technologies may 6\n",
      "foreseeable technologies 6\n",
      "used gleaming 6\n",
      "clothed figures 6\n",
      "\n",
      "35\n",
      "\n",
      "forest spurned 4\n",
      "bram forest 4\n",
      "spurned violence 4\n",
      "stars planets 3\n",
      "rolling drums 2\n",
      "\n",
      "36\n",
      "\n",
      "nineteenth century 6\n",
      "twentieth century 5\n",
      "inventing airplane 2\n",
      "intense spectators 2\n",
      "spectators began 2\n",
      "\n",
      "37\n",
      "\n",
      "millions upon 7\n",
      "protelectricity fourth 6\n",
      "rays millions 6\n",
      "fourth order 6\n",
      "millions eons 6\n",
      "\n",
      "38\n",
      "\n",
      "upon flat 2\n",
      "never never 2\n",
      "wheeling beyond 2\n",
      "wind ran 2\n",
      "forementioned carlowitz 2\n",
      "\n",
      "39\n",
      "\n",
      "atomic energy 3\n",
      "spacecraft low 2\n",
      "energy commission 2\n",
      "north america 2\n",
      "parallelled homo 2\n",
      "\n",
      "40\n",
      "\n",
      "brave men 5\n",
      "however conscience 3\n",
      "conscience smote 3\n",
      "smote sorely 3\n",
      "sorely much 3\n",
      "\n",
      "41\n",
      "\n",
      "vice president 3\n",
      "pencils force 3\n",
      "council assembled 3\n",
      "assembled vice 3\n",
      "streams pencils 3\n",
      "\n",
      "42\n",
      "\n",
      "crew members 6\n",
      "phone call 3\n",
      "shot holed 2\n",
      "holed helmet 2\n",
      "fruit taking 2\n",
      "\n",
      "43\n",
      "\n",
      "double crews 4\n",
      "crews duty 4\n",
      "made incongruous 3\n",
      "incongruous pale 3\n",
      "bart lingered 3\n",
      "\n",
      "44\n",
      "\n",
      "type condensers 6\n",
      "condensers built 6\n",
      "special type 6\n",
      "ghost toast 3\n",
      "makes ghost 3\n",
      "\n",
      "45\n",
      "\n",
      "utter silence 5\n",
      "small amounts 5\n",
      "parties well 2\n",
      "amounts favor 2\n",
      "cocktail parties 2\n",
      "\n",
      "46\n",
      "\n",
      "tree trunk 4\n",
      "individuals escaped 4\n",
      "tree trunks 4\n",
      "stark tree 3\n",
      "escaped destruction 3\n",
      "\n",
      "47\n",
      "\n",
      "good fortune 11\n",
      "good humor 8\n",
      "good health 6\n",
      "freak good 3\n",
      "stimulating heart 2\n",
      "\n",
      "48\n",
      "\n",
      "molten metal 7\n",
      "metal plates 3\n",
      "metal plate 3\n",
      "jets greenish 3\n",
      "greenish flame 3\n",
      "\n",
      "49\n",
      "\n",
      "last month 3\n",
      "chumkt rebelled 3\n",
      "vantage point 3\n",
      "notes pointed 3\n",
      "sentences similar 3\n",
      "\n",
      "50\n",
      "\n",
      "atomic bomb 8\n",
      "men first 4\n",
      "floating men 4\n",
      "hundred percent 3\n",
      "conversion atomic 3\n",
      "\n",
      "51\n",
      "\n",
      "clenched fist 7\n",
      "fists clenched 3\n",
      "bridget fists 2\n",
      "opened clenched 2\n",
      "brandishing clenched 2\n",
      "\n",
      "52\n",
      "\n",
      "snatched remaining 5\n",
      "krag snatched 5\n",
      "remaining egg 5\n",
      "twin breasts 4\n",
      "raw material 4\n",
      "\n",
      "53\n",
      "\n",
      "year year 4\n",
      "quiet nearly 3\n",
      "nearly dark 3\n",
      "dining room 3\n",
      "dark dining 3\n",
      "\n",
      "54\n",
      "\n",
      "shearing feet 4\n",
      "right track 4\n",
      "vague idea 4\n",
      "backed away 3\n",
      "yards away 3\n",
      "\n",
      "55\n",
      "\n",
      "fourth dimension 2\n",
      "certain heavens 1\n",
      "spectator hear 1\n",
      "fortyeight hours 1\n",
      "blend accord 1\n",
      "\n",
      "56\n",
      "\n",
      "loud voice 6\n",
      "cries hoarse 4\n",
      "animal cries 4\n",
      "hoarse voices 4\n",
      "hoarse voice 4\n",
      "\n",
      "57\n",
      "\n",
      "large scale 8\n",
      "larger scale 5\n",
      "drugs transport 3\n",
      "transport criminals 3\n",
      "beneath reclining 3\n",
      "\n",
      "58\n",
      "\n",
      "san francisco 3\n",
      "eyes cleared 3\n",
      "straightened strangely 3\n",
      "colored eyes 3\n",
      "strength hairless 3\n",
      "\n",
      "59\n",
      "\n",
      "intelligent life 10\n",
      "irrigation systems 5\n",
      "meant intelligent 5\n",
      "intelligent beings 5\n",
      "systems meant 5\n",
      "\n",
      "60\n",
      "\n",
      "chawk chawk 15\n",
      "public opinion 5\n",
      "charawk chawk 4\n",
      "spring scales 4\n",
      "chawk shrieked 4\n",
      "\n",
      "61\n",
      "\n",
      "force majeure 4\n",
      "recognized force 4\n",
      "stanton recognized 3\n",
      "hidden view 3\n",
      "men lives 2\n",
      "\n",
      "62\n",
      "\n",
      "united states 6\n",
      "fired like 2\n",
      "trucks two 2\n",
      "might fired 2\n",
      "like imprisoned 2\n",
      "\n",
      "63\n",
      "\n",
      "fat man 26\n",
      "haired man 15\n",
      "honest man 8\n",
      "man said 5\n",
      "spearman teased 5\n",
      "\n",
      "64\n",
      "\n",
      "etext produced 8\n",
      "fantastic universe 7\n",
      "produced fantastic 7\n",
      "note etext 4\n",
      "notes etext 4\n",
      "\n",
      "65\n",
      "\n",
      "single file 6\n",
      "glance shifted 5\n",
      "upward scanning 5\n",
      "shifted upward 5\n",
      "lance glance 5\n",
      "\n",
      "66\n",
      "\n",
      "control panel 20\n",
      "instrument panel 4\n",
      "facing control 4\n",
      "panel oversized 2\n",
      "panel damonscope 2\n",
      "\n",
      "67\n",
      "\n",
      "total darkness 5\n",
      "provincial mayor 3\n",
      "enemy also 2\n",
      "upon hated 2\n",
      "tiny identical 2\n",
      "\n",
      "68\n",
      "\n",
      "interstellar space 13\n",
      "flight eastbourne 3\n",
      "outer space 3\n",
      "relate latest 3\n",
      "latest news 3\n",
      "\n",
      "69\n",
      "\n",
      "fourth dimension 7\n",
      "previous day 5\n",
      "previous night 3\n",
      "old chap 2\n",
      "lennard first 2\n",
      "\n",
      "70\n",
      "\n",
      "long legs 4\n",
      "long pause 4\n",
      "accustomed long 3\n",
      "era several 2\n",
      "long row 2\n",
      "\n",
      "71\n",
      "\n",
      "damned thing 11\n",
      "funny thing 9\n",
      "dear said 5\n",
      "said bensington 4\n",
      "trail damned 3\n",
      "\n",
      "72\n",
      "\n",
      "machine shop 6\n",
      "considerable distance 4\n",
      "spent little 3\n",
      "little time 3\n",
      "spent considerable 3\n",
      "\n",
      "73\n",
      "\n",
      "square inch 8\n",
      "per square 7\n",
      "pounds per 6\n",
      "thousand years 4\n",
      "second steam 4\n",
      "\n",
      "74\n",
      "\n",
      "blond hair 24\n",
      "cropped blond 5\n",
      "close cropped 5\n",
      "feathery blond 4\n",
      "tumbled blond 4\n",
      "\n",
      "75\n",
      "\n",
      "per cent 21\n",
      "nationwise adult 4\n",
      "artificial gravity 4\n",
      "ticklerization per 4\n",
      "underground ticklerization 4\n",
      "\n",
      "76\n",
      "\n",
      "russian frontier 3\n",
      "phobar announcement 3\n",
      "announcement made 3\n",
      "shocking discovery 3\n",
      "frontier shocking 3\n",
      "\n",
      "77\n",
      "\n",
      "precious stones 8\n",
      "metals fine 2\n",
      "nevia neither 2\n",
      "evident nevia 2\n",
      "stones burnished 2\n",
      "\n",
      "78\n",
      "\n",
      "civil war 4\n",
      "would continue 3\n",
      "world federation 3\n",
      "engaged preparing 3\n",
      "peace plan 3\n",
      "\n",
      "79\n",
      "\n",
      "yaaa rhinoceros 2\n",
      "mightiest corporations 2\n",
      "trust funds 2\n",
      "expenses quadrupled 2\n",
      "matched daggers 2\n",
      "\n",
      "80\n",
      "\n",
      "phildee mother 8\n",
      "green grass 8\n",
      "mother wanted 8\n",
      "wanted green 8\n",
      "applied without 5\n",
      "\n",
      "81\n",
      "\n",
      "human nature 4\n",
      "ignored kind 2\n",
      "hellman ignored 2\n",
      "beings work 1\n",
      "moved closer 1\n",
      "\n",
      "82\n",
      "\n",
      "passing blazed 4\n",
      "blazed masque 4\n",
      "masque beauty 4\n",
      "bread one 3\n",
      "gnawed crust 3\n",
      "\n",
      "83\n",
      "\n",
      "per cent 5\n",
      "san francisco 4\n",
      "hank kuran 3\n",
      "intensely coloured 3\n",
      "right angles 3\n",
      "\n",
      "84\n",
      "\n",
      "fairy tales 4\n",
      "brutality humour 3\n",
      "doth never 3\n",
      "never hasty 3\n",
      "extended straight 3\n",
      "\n",
      "85\n",
      "\n",
      "lower lip 10\n",
      "conscious mind 8\n",
      "ross conscious 4\n",
      "lonnie lower 4\n",
      "slowly lonnie 4\n",
      "\n",
      "86\n",
      "\n",
      "white teeth 20\n",
      "white beard 9\n",
      "life long 6\n",
      "beauty unsurpassed 6\n",
      "railroad accuracy 6\n",
      "\n",
      "87\n",
      "\n",
      "keen eyes 8\n",
      "beady eyes 4\n",
      "connel leveled 3\n",
      "beady eye 3\n",
      "heart bounding 3\n",
      "\n",
      "88\n",
      "\n",
      "salt water 6\n",
      "replaced brown 4\n",
      "lichtenstein books 4\n",
      "lady lichtenstein 4\n",
      "books replaced 4\n",
      "\n",
      "89\n",
      "\n",
      "patrol ships 10\n",
      "retired patrol 6\n",
      "conferences pilots 3\n",
      "ships conferences 3\n",
      "cried gleefully 3\n",
      "\n",
      "90\n",
      "\n",
      "hotel room 10\n",
      "dining room 9\n",
      "conference room 3\n",
      "darkness falling 2\n",
      "white room 2\n",
      "\n",
      "91\n",
      "\n",
      "idiot general 4\n",
      "walked briskly 4\n",
      "resignation forms 4\n",
      "general criswell 4\n",
      "criswell walked 4\n",
      "\n",
      "92\n",
      "\n",
      "collected huge 3\n",
      "plant life 3\n",
      "cultivated plant 3\n",
      "specially cultivated 3\n",
      "crackpots get 2\n",
      "\n",
      "93\n",
      "\n",
      "san francisco 3\n",
      "guinea pigs 3\n",
      "went steadily 2\n",
      "preparations went 2\n",
      "channel preparations 2\n",
      "\n",
      "94\n",
      "\n",
      "magnetic field 7\n",
      "magnetic fields 5\n",
      "heat light 3\n",
      "effect heat 3\n",
      "fields effect 3\n",
      "\n",
      "95\n",
      "\n",
      "eastern horizon 3\n",
      "away inordinate 2\n",
      "inordinate amounts 2\n",
      "lost wailed 2\n",
      "spouts earthen 2\n",
      "\n",
      "96\n",
      "\n",
      "fuel tanks 7\n",
      "san francisco 6\n",
      "prime minister 5\n",
      "smashed fuel 3\n",
      "sparks fire 3\n",
      "\n",
      "97\n",
      "\n",
      "cybrain pumped 3\n",
      "pumped orders 3\n",
      "lieutenant gripping 2\n",
      "oglethorpe made 2\n",
      "close hand 2\n",
      "\n",
      "98\n",
      "\n",
      "desk drawer 4\n",
      "fish plentifully 3\n",
      "plentifully equipped 3\n",
      "goggling eyes 3\n",
      "huge goggling 3\n",
      "\n",
      "99\n",
      "\n",
      "wild beasts 9\n",
      "sunk furnaces 4\n",
      "mines sunk 4\n",
      "furnaces blown 4\n",
      "blown smelters 3\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from collections import defaultdict, Counter\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    " \n",
    "sw = set(stopwords.words('english'))\n",
    "    \n",
    "for a in range(0, len(all_topic_sentences)):\n",
    "    \n",
    "    tokens = []\n",
    "    for t in re.split('[^a-z]', ' '.join(all_topic_sentences[a]).lower()):\n",
    "        if t > '' and t not in sw:\n",
    "             tokens.append(t)\n",
    "    \n",
    "    token_count = defaultdict(int)\n",
    "                \n",
    "    for b in range(0, len(tokens) - 1):\n",
    "        token_count[' '.join(tokens[b: b + 2])] += 1\n",
    "                \n",
    "    print\n",
    "    print a\n",
    "    print\n",
    "    \n",
    "    for w in Counter(token_count).most_common(5):\n",
    "        print w[0], w[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!\n"
     ]
    }
   ],
   "source": [
    "for a in range(0, len(all_topic_sentences)):\n",
    "    \n",
    "    f = codecs.open('raw_stories/' + str(a) + '.txt', 'w', encoding='utf-8')\n",
    "    f.write('\\n\\n'.join(all_topic_sentences[a]))\n",
    "    f.close()\n",
    "    \n",
    "print 'Done!'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
